<!doctype html>
<html>
<head>
<meta charset="utf-8"/>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>I  | High Performance Web Sites</title>
<link rel="stylesheet" href="../css/article.css" />
</head>
<body>
<div class="m-content">
<h1>I  | High Performance Web Sites</h1>
<div>
            
			<p>April 26, 2013 10:08 am | <a href="http://www.stevesouders.com/blog/2013/04/26/i/#comments" title="Comment on I ">17 Comments</a></p><!-- .entry-meta -->

			<p>Much of my work on web performance has focused on JavaScript and CSS, starting with the early rules&nbsp;<a href="http://developer.yahoo.com/blogs/ydn/high-performance-sites-rule-6-move-scripts-bottom-7200.html">Move Scripts to the Bottom</a> and <a href="http://developer.yahoo.com/blogs/ydn/high-performance-sites-rule-5-put-stylesheets-top-7197.html">Put Stylesheets at the Top</a>&nbsp;from back in 2007(!). To emphasize these best practices I used to say, “JS and CSS are the most important bytes in the page”.</p>
<p>A few months ago I realized that wasn’t true. <em>Images are the most important bytes in the page.</em></p>
<p>My focus on JS and CSS was largely motivated by the desire to get the images downloaded as soon as possible. Users see images. They don’t <em>see</em> JS and CSS. It is true that JS and CSS affect what is seen in the page, and even whether and how images are displayed (e.g., JS photo carousels, and CSS background images and media queries). But my realization was JS and CSS are the means by which we get to these images. During page load we want to get the JS and CSS out of the way as quickly as possible so that the images (and text) can be shown.</p>
<p><em>My main motivation for optimizing JS and CSS is to get rendering to happen as quickly as possible.</em></p>
<h3>Rendering starts very late</h3>
<p>With this focus on rendering in mind, I went to the <a href="http://httparchive.org/">HTTP Archive</a> to see how quickly we’re getting pages to render. The HTTP Archive runs on top of <a href="http://www.webpagetest.org/">WebPagetest</a> which reports the following time measurements:</p>
<ul>
<li>time-to-first-byte (TTFB) – When the first packet of the HTML document arrives.</li>
<li>start render – When the page starts rendering.</li>
<li>onload – When window.onload fires.</li>
</ul>
<p>I extracted the 50th and 90th percentile values for these measurements across the world’s top 300K URLs. As shown, <em>nothing is rendered for the first third of page load time!</em><i><br /></i></p>
<table>
<tbody>
<tr>
<td colspan="4">Table 1. Time milestones during page load</td>
</tr>
<tr>
<th></th>
<th>TTFB</th>
<th>start render</th>
<th>onload</th>
</tr>
<tr>
<td>50th percentile</td>
<td>610 ms</td>
<td>2227 ms</td>
<td>6229 ms</td>
</tr>
<tr>
<td>90th percentile</td>
<td>1780 ms</td>
<td>5112 ms</td>
<td>15969 ms</td>
</tr>
</tbody>
</table>
<h3><span>Preloading</span></h3>
<p>The fact that rendering doesn’t start until the page is 1/3 into the overall page load time is eye-opening. Looking at both the 50th and 90th percentile stats from the HTTP Archive, rendering starts ~32-36% into the page load time. It takes ~10% of the overall page load time to get the first byte. Thus, for ~22-26% of the page load time the browser has bytes to process but nothing is drawn on the screen. During this time the browser is typically downloading and parsing scripts and stylesheets – both of which block rendering on the page.</p>
<p>It used to be that the browser was largely idle during this early loading phase (after TTFB and before start render). That’s because when an older browser started downloading a script,&nbsp;<a title="High Performance Web Sites: Rule 6  Move Scripts to the Bottom" href="http://developer.yahoo.com/blogs/ydn/high-performance-sites-rule-6-move-scripts-bottom-7200.html">all other downloads were blocked</a>. This is still visible in&nbsp;<a title="Browserscope" href="http://www.browserscope.org/?category=network&amp;v=1&amp;ua=IE%206,IE%207,IE%208,IE%209,IE%2010">IE 6&amp;7</a>. Browser vendors realized that while it’s true that constructing the DOM has to wait for a script to download and execute, there’s no reason other resources deeper in the page couldn’t be&nbsp;<em>fetched</em> in parallel. Starting with IE 8 in 2009, browsers started looking past the currently downloading script for other resources (i.e, SCRIPT, IMG, LINK, and IFRAME tags) and <em>preloading</em> those requests in parallel. One study showed preloading makes&nbsp;<a title="The WebKit PreloadScanner" href="http://gent.ilcore.com/2011/01/webkit-preloadscanner.html">pages load ~20% faster</a>. Today, all major browsers support preloading. In these&nbsp;<a href="http://www.browserscope.org/?category=network&amp;v=1&amp;ua=Chrome%202,Firefox%202,IE%208,iPhone%202,Opera%2012,Opera%20Mobile%2010,Safari%201">Browserscope results</a>&nbsp;I show the earliest version of each major browser where preloading was first supported.</p>
<p>(As an aside, I think preloading is the single biggest performance improvement browsers have ever made. Imagine today, with the abundance of scripts on web pages, what performance would be like if each script was downloaded sequentially and blocked all other downloads.)</p>
<h3>Preloading and responsive images</h3>
<p>This ties back to this <a href="https://twitter.com/grigs/status/327429827726561280">tweet from Jason Grigsby</a>:</p>
<blockquote><p>I’ll be honest. I’m tired of pushing for resp images and increasingly inclined to encourage devs to use JS to simply break pre-loaders.</p></blockquote>
<p>The “resp images” Jason refers to are techniques by which image requests are generated by JavaScript. This is generally used to adapt the size of images for different screen sizes. One example is <a href="https://github.com/scottjehl/picturefill">Picturefill</a>. When you combine “pre-loaders” and “resp images” an issue arises – the preloader looks ahead for IMG tags and fetches their SRC, but responsive image techniques typically don’t have a SRC, or have a stub image such as a 1×1 transparent pixel. This defeats the benefits of preloading for images. So there’s a tradeoff:</p>
<ul>
<li><span>Don’t use responsive images so that the preloader can start downloading images sooner, but the images might be larger than needed for the current device and thus take longer to download (and cost more for limited cellular data plans).</span></li>
<li>Use responsive images which doesn’t take advantage of preloading which means the images are loaded later after the required JS is downloaded and executed, and the IMG DOM elements have been created.</li>
</ul>
<p>As Jason says in a follow-up <a href="https://twitter.com/grigs/status/327430930895609858">tweet</a>:</p>
<blockquote><p>The thing that drives me nuts is that almost none of it has been tested. Lots of gospel, not a lot of data.</p></blockquote>
<p>I don’t have any data comparing the two tradeoffs, but the HTTP Archive data showing that rendering doesn’t start until 1/3 into page load is telling. It’s likely that rendering is being blocked by scripts, which means the IMG DOM elements haven’t been created yet. So at some point after the 1/3 mark the IMG tags are parsed and at some point after that the responsive image JS executes and starts downloading the necessary images.</p>
<p>In my opinion, this is too late in the page load process to initiate the image requests, and will likely cause the web page to render later than it would if the preloader was used to download images. Again, I don’t have data comparing the two techniques. Also, I’m not sure how the preloader works with the responsive image techniques done via markup. (Jason has a blog post that touches on that,&nbsp;<a href="http://blog.cloudfour.com/the-real-conflict-behind-picture-and-srcset/">The real conflict behind &lt;picture&gt; and @srcset</a>.)</p>
<p>Ideally we’d have a responsive image solution in markup that would work with preloaders. Until then, I’m nervous about recommending to the dev community at large to move toward responsive images at the expense of defeating preloading. I expect browsers will add more benefits to preloading, and I’d like websites to be able to take advantage of those benefits both now and in the future.</p>


            
            			
		</div></div>
<script>var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-34802167-1']); _gaq.push(['_setDomainName', 'liuzhe.co']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })();</script></body>
</html>