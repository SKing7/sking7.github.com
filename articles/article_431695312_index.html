<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="keywords" content="web performance faster web sites high performance web sites website optimization best practices javascript css web development mobile">
<title>Domain Sharding revisited | High Performance Web Sites</title>
<link rel="stylesheet" type="text/css" media="all" href="http://www.stevesouders.com/blog/wp-content/themes/SteveSouders/style.css" />
<link rel="alternate" type="application/rss+xml" title="High Performance Web Sites &raquo; Domain Sharding revisited Comments Feed" href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/feed/" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.stevesouders.com/blog/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.stevesouders.com/blog/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='Web performance for the future' href='http://www.stevesouders.com/blog/2013/08/27/web-performance-for-the-future/' />
<meta name="generator" content="WordPress 3.6.1" />
<link rel='canonical' href='http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/' />
<link rel='shortlink' href='http://www.stevesouders.com/blog/?p=3720' />
<script type="text/javascript">
	window._wp_rp_static_base_url = 'http://dtmvdvtzf8rz0.cloudfront.net/static/';
	window._wp_rp_wp_ajax_url = "http://www.stevesouders.com/blog/wp-admin/admin-ajax.php";
	window._wp_rp_plugin_version = '2.6';
	window._wp_rp_post_id = '3720';
	window._wp_rp_num_rel_posts = '5';
</script>

<script>
(function(d, s) {
	var js = d.getElementsByTagName(s)[0],
		ln = d.createElement(s);
	ln.src = '//lognormal.net/boomerang/8fa4fc2785a82fae20c596ca4f45d1b218a9ab12ffef741d09b29532';
	js.parentNode.insertBefore(ln, js);
})(document, 'script');
</script>

<script type="text/javascript">
// GOOGLE ANALYTICS
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-15026169-1']);
_gaq.push(['_setSiteSpeedSampleRate', 100]);
_gaq.push(['_trackPageview']);

(function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>
</head>

<body>
<div id="header">
	<div class="containerwrapper">
  		<h1 id="logo"><a href="http://stevesouders.com/"><span>SteveSouders.com</span></a></h1>
        <ul id="topnav">
        <li><a href="http://stevesouders.com/about.php">about</a></li>
        <li><a href="http://stevesouders.com/contact.php">contact</a></li>
        <li class="last"> <a href="http://twitter.com/#!/souders/"><img src="/images/twitter-icon.png" width=16 height=16 border=0 style="vertical-align: bottom;"></a>
        </ul>
	</div>    
</div>
<div id="contentWrapper" class="containerwrapper">
<div id="sidebar">
<div id="search-3" class="widget widget_search"><form role="search" method="get" id="searchform" class="searchform" action="http://www.stevesouders.com/blog/">
				<div>
					<label class="screen-reader-text" for="s">Search for:</label>
					<input type="text" value="" name="s" id="s" />
					<input type="submit" id="searchsubmit" value="Search" />
				</div>
			</form></div><div id="custom-recent-posts-3" class="widget custom-recent-posts"><h3 class="blockheader">Most Recent Posts</h3>		<ul style="list-style-type: none; margin-left: 20px; text-indent: -20px;">
        		<li><a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/" rel="bookmark">Domain Sharding revisited</a></li>
				<li><a href="http://www.stevesouders.com/blog/2013/08/27/web-performance-for-the-future/" rel="bookmark">Web performance for the future</a></li>
				<li><a href="http://www.stevesouders.com/blog/2013/07/16/activetable-bookmarklet/" rel="bookmark">ActiveTable bookmarklet</a></li>
				<li><a href="http://www.stevesouders.com/blog/2013/07/15/twitter-widget-update/" rel="bookmark">Twitter widget update</a></li>
				<li><a href="http://www.stevesouders.com/blog/2013/06/16/browser-busy-indicators/" rel="bookmark">Browser Busy Indicators</a></li>
				</ul>
		<a href="http://www.stevesouders.com/blog/archives/">View Archive</a></div><div id="text-3" class="widget widget_text"><h3 class="blockheader">Feeds</h3>			<div class="textwidget"><a href="http://www.stevesouders.com/blog/feed/"><img src="/blog/wp-content/uploads/2011/02/feed.png" width="10" height="10" /> </a><a href="http://www.stevesouders.com/blog/feed/">Posts</a><br />
<a href="http://www.stevesouders.com/blog/comments/feed/"><img src="/blog/wp-content/uploads/2011/02/feed.png" width="10" height="10" /> </a><a href="http://www.stevesouders.com/blog/comments/feed/">Comments</a></li></div>
		</div>        
</div><div id="content">



		<div id="post-3720" class="post-3720 post type-post status-publish format-standard hentry category-evangelism category-http category-http-archive category-performance category-web-development tag-domain-sharding tag-spdy">
        	            	<h1>Domain Sharding revisited</h1>
            
			<p class="meta">September 5, 2013 10:41 am | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comments" title="Comment on Domain Sharding revisited">12 Comments</a></p><!-- .entry-meta -->

			<p>With the adoption of SPDY and progress on HTTP 2.0, I hear some people referring to <a href="http://www.stevesouders.com/blog/2009/05/12/sharding-dominant-domains/">domain sharding</a> as a performance anti-pattern. I disagree. Sharding resources across multiple domains is a major performance win for many websites. However, there is room for debate. Domain sharding isn&#8217;t appropriate for everyone. It may hurt performance if done incorrectly. And it&#8217;s utility might be short-lived. Is it worth sharding domains? Let&#8217;s take a look.</p>
<h3>Compelling Data</h3>
<p>The <a href="http://httparchive.org/">HTTP Archive</a> has a field call &#8220;maxDomainReqs&#8221;. The explanation requires a few sentences: Websites request resources from various domains. The average website today accesses 16 different domains as shown in the chart below. That number has risen from 12.5 a year ago. That&#8217;s not surprising given the rise in third party content (ads, widgets, analytics).</p>
<p><a href="http://httparchive.org/trends.php#numDomains&amp;maxDomainReqs"><img class="aligncenter" alt="" src="http://stevesouders.com/images/ha-domains.png" width="490" /></a></p>
<p>The HTTP Archive counts the number of requests made on each domain. The domain with the most requests is the &#8220;max domain&#8221; and the number of requests on that domain is the &#8220;maxDomainReqs&#8221;. The average maxDomainReqs value has risen from 47 to 50 over the past year. That&#8217;s not a huge increase, but the fact that the average number of requests on one domain is so high is startling.</p>
<p>50 is the <em>average</em> maxDomainReqs across the world&#8217;s top 300K URLs. But averages don&#8217;t tell the whole story. Using the <a href="http://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/">HTTP Archive data in BigQuery</a> and <a href="http://bigqueri.es/">bigqueri.es</a>, both created by Ilya Grigorik, it&#8217;s easy to find <a href="http://bigqueri.es/t/max-requests-to-one-hostname/67">percentile values for maxDomainReqs</a>: the 50th percentile is 39, the 90th percentile is 97, and the 95th percentile is 127 requests on a single domain.</p>
<p>This data shows that a majority of websites have 39 or more resources being downloaded from a single domain. Most browsers do <a href="http://www.browserscope.org/?category=network&amp;v=1">six requests per hostname</a>. If we evenly distribute these 39 requests across the connections, each connection must do 6+ sequential requests. Response times per request vary widely, but I use 500 ms as an optimistic estimate. If we use 500 ms as the typical responsive time, this introduces a 3000 ms long pole in the response time tent. In reality, requests are assigned to whatever connection is available, and 500 ms might not be the typical response time for your requests. But given the six-connections-per-hostname limit, 39 requests on one domain is a lot.</p>
<h3>Wrong sharding</h3>
<p>There are costs to domain sharding. You&#8217;ll have to modify your website to actually do the sharding. This is likely a one time cost; the infrastructure only has to be setup once. In terms of performance the biggest cost is the extra DNS lookup for each new domain. Another performance cost is the overhead of establishing each TCP connection and ramping up its congestion window size.</p>
<p>Despite these costs, domain sharding has great benefit for websites that need it and do it correctly. That first part is important &#8211; it doesn&#8217;t make sense to do domain sharding if your website has a low &#8220;maxDomainReqs&#8221; value. For example, if the maximum number of resources downloaded on a single domain is 6, then you shouldn&#8217;t deploy domain sharding. With only 6 requests on a single domain, most browsers are able to download all 6 in parallel. On the other hand, if you have 39 requests on a single domain, sharding is probably a good choice. So where&#8217;s the cutoff between 6 and 39? I don&#8217;t have data to answer this, but I would say 20 is a good cutoff. Other aspects of the page affect this decision. For example, if your page has a lot of other requests, then those 20 resources might not be the long pole in the tent.</p>
<p>The success of domain sharding can be mitigated if it&#8217;s done incorrectly. It&#8217;s important to keep these guidelines in mind.</p>
<ul>
<li>It&#8217;s best to shard across only two domains. You can test larger values, but previous tests show two to be the optimal choice.</li>
<li>Make sure that the sharding logic is consistent for each resource. You don&#8217;t want a single resource, say main.js, to flip-flop between domain1 and domain2.</li>
<li>You don&#8217;t need to setup different servers for each domain &#8211; just create CNAMEs. The browser doesn&#8217;t care about the final IP address &#8211; it only cares that the hostnames are different.</li>
</ul>
<p>These and other issues are explained in more detail in Chapter 11 of <a title="Amazon" href="http://www.amazon.com/Even-Faster-Web-Sites-Performance/dp/0596522304/">Even Faster Web Sites</a>.</p>
<h3>Short term hack?</h3>
<p>Perhaps the strongest argument against domain sharding is that it&#8217;s unnecessary in the world of <a href="http://www.chromium.org/spdy">SPDY</a> (as well as HTTP 2.0). In fact, domain sharding probably hurts performance under SPDY. SPDY supports concurrent requests (send all the request headers early) as well as request prioritization. Sharding across multiple domains diminishes these benefits. SPDY is supported by Chrome, Firefox, Opera, and IE 11. If your traffic is dominated by those browsers, you might want to skip domain sharding. On the other hand, IE 6&amp;7 are still somewhat popular and only support 2 connections per hostname, so domain sharding is an even bigger win in those browsers.</p>
<p>A middle ground is to alter domain sharding depending on the client: 1 domain for browsers that support SPDY, 2 domains for non-SPDY modern browsers, 3-4 domains for IE 6-7. This makes domain sharding harder to deploy. It also lowers the cache hit rate on intermediate proxies.</p>
<p>There&#8217;s no need for domain sharding in the world of HTTP 2.0 across all popular browsers. Until then, there&#8217;s no silver bullet answer. But if you&#8217;re one of the websites with 39+ resources on a single hostname, domain sharding is worth exploring.</p>

<div class="wp_rp_wrap  wp_rp_momma" id="wp_rp_first"><div class="wp_rp_content"><h3 class="related_post_title">Related Posts</h3><ul class="related_post wp_rp" style="visibility: visible"><li ><a href="http://www.stevesouders.com/blog/2009/05/12/sharding-dominant-domains/" class="wp_rp_title">Sharding Dominant Domains</a></li><li ><a href="http://www.stevesouders.com/blog/2009/11/12/speedy-twice-as-fast-as-http/" class="wp_rp_title">SPeeDY &#8211; twice as fast as HTTP</a></li><li ><a href="http://www.stevesouders.com/blog/2010/02/15/browser-performance-wishlist/" class="wp_rp_title">Browser Performance Wishlist</a></li><li ><a href="http://www.stevesouders.com/blog/2011/04/18/http-archive-max-ag/" class="wp_rp_title">HTTP Archive: max-age</a></li><li ><a href="http://www.stevesouders.com/blog/2011/05/09/http-archive-servers-and-most-404s/" class="wp_rp_title">HTTP Archive: servers and most 404s</a></li></ul><div class="wp_rp_footer"><a class="wp_rp_backlink" target="_blank" rel="nofollow" href="http://www.zemanta.com/?wp-related-posts">Zemanta</a></div></div></div>
            
            			
		</div><!-- #post-## -->


		
			<div id="comments">

			<h3 id="comments-title">12 Responses to <em>Domain Sharding revisited</em></h3>


			<ol class="commentlist">
				<li class="comment even thread-even depth-1" id="li-comment-60142">
<div id="comment-60142">
<p class="comment-metadata">
<strong><a href='http://andydavies.me' rel='external nofollow' class='url'>Andy Davies</a></strong> | 05-Sep-13 at 11:02 am | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60142">Permalink</a> |  </p>


<p>Will Chan talked about the network congestion produced by too many shards here &#8211; <a href="https://insouciant.org/tech/network-congestion-and-web-browsing/#Google_Kitten_Search" rel="nofollow">https://insouciant.org/tech/network-congestion-and-web-browsing/#Google_Kitten_Search</a></p>
<p>The post has a really interesting line that Chrome is already limiting parallelism for some types of resources &#8220;Indeed, we’ve experimented with doing so and found compelling data to support limiting the number of concurrent image requests to 10, which should roll out to stable channel in the upcoming Chrome 27 release and dramatically mitigate congestion related issues.&#8221;</p>
</div>
</li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="li-comment-60144">
<div id="comment-60144">
<p class="comment-metadata">
<strong><a href='http://www.softwarearchiblog.com/' rel='external nofollow' class='url'>Lior Bar-On</a></strong> | 05-Sep-13 at 12:30 pm | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60144">Permalink</a> |  </p>


<p>Repost due to some bad edits:</p>
<p>Hi Steve,</p>
<p>Thanks for the interesting article!</p>
<p>According to what I know, the HTTP standard recommended a limit of 2 connections per host – a limit no modern browser does comply to, anymore. browsers did select to use no more 6-8 parallel connections per host to balance between Parallelism and Congestion. As far as I read, this number is based on exhaustive testing.</p>
<p>If browser makers would believe 10 or even 20 parallel connections per host is preferable, I believe they would have just done that.</p>
<p>I understand the browser doesn&#8217;t know how many requests are expected per domain, while the site’s developer does. But… if the median site has 39 requests + I make a guess (from sites I know) almost all of the other domain require less than 6 request – it sound logical that increasing the browser amount of connections per domain would provide a benefit here. Yeah… this is just guessing.</p>
<p>Is there any data available about the effect of “max domain sharding” on different web sites?</p>
<p>Thanks in advance,<br />
Lior</p>
</div>
</li><!-- #comment-## -->
<li class="comment byuser comment-author-sowrock bypostauthor even thread-even depth-1 author" id="li-comment-60145">
<div id="comment-60145">
<p class="comment-metadata">
<strong><a href='http://stevesouders.com' rel='external nofollow' class='url'>Steve Souders</a></strong> | 05-Sep-13 at 1:07 pm | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60145">Permalink</a> |  </p>


<p>Andy: Thanks for the pointer to Will&#8217;s (awesome) article. Note that this only applies to images, which are given a lower download priority than scripts &#038; stylesheets. Thus, a website might still benefit from domain sharding wrt non-image assets and to fill that queue of 10 image requests when it&#8217;s time.</p>
<p>Lior: It is possible that browsers may revisit their max-connections-per-domain settings. Opera dropped there&#8217;s from 8 connections in Opera 8 down to 6 connections starting in Opera 11. It looks like IE 10 raised its value from 6 to 8 connections. Domain sharding is a way for the web developer to work with the browser to decide what&#8217;s best. Both are important. I&#8217;m not aware of any public data on this topic.</p>
</div>
</li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="li-comment-60146">
<div id="comment-60146">
<p class="comment-metadata">
<strong>Patrick McManus</strong> | 05-Sep-13 at 9:45 pm | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60146">Permalink</a> |  </p>


<p>Nice post Steve &#8211; a couple additional thoughts:</p>
<p>1] the size of the resources being transferred should be a primary factor in deciding whether or not to shard aggressively. Small resources should be sharded more aggressively. Sharding large hi-res images is a great way to create self induced packet loss and retransmissions, but not sharding buttons and icons is a great way to leave unused bandwidth on the table.</p>
<p>2] Both chrome and firefox will automatically unshard transparently for you when using spdy and both of the sharded hosts are at the same IP address and covered under one SSL cert (e.g. *.example.com). That&#8217;s pretty cool because you can automatically shard for HTTP/1 but not for HTTP/2 (spdy) without doing UA sniffing. Cause nothing ever goes wrong with UA sniffing :)</p>
<p>3] At high levels of sharding-induced parallelism firefox will actually engage in pacing requests a few milliseconds apart.. they are staggered but by a factor much smaller than a RTT. This really helps with the packet loss problem.</p>
</div>
</li><!-- #comment-## -->
<li class="comment byuser comment-author-sowrock bypostauthor even thread-even depth-1 author" id="li-comment-60147">
<div id="comment-60147">
<p class="comment-metadata">
<strong><a href='http://stevesouders.com' rel='external nofollow' class='url'>Steve Souders</a></strong> | 06-Sep-13 at 9:06 am | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60147">Permalink</a> |  </p>


<p>Patrick: Great to hear from you! I didn&#8217;t know about Chrome &#038; Firefox UNsharding &#8211; that&#8217;s very cool. And Firefox&#8217;s staggered multiple requests is great. Were those changes motivated by sharding specifically, or do those issues arise on popular websites even without sharding?</p>
</div>
</li><!-- #comment-## -->
<li class="comment odd alt thread-odd thread-alt depth-1" id="li-comment-60148">
<div id="comment-60148">
<p class="comment-metadata">
<strong><a href='http://www.finanznachrichten.de' rel='external nofollow' class='url'>Markus</a></strong> | 07-Sep-13 at 11:22 am | <a href="http://www.stevesouders.com/blog/2013/09/05/domain-sharding-revisited/#comment-60148">Permalink</a> |  </p>


<p>Thank you for the interesting article, Steve, but I disagree with your view of SPDY.</p>
<p>SPDY requires a https connection which means that all those people using Internet Explorers will have see that the website is slower. So, using SPDY might bring a little improvement for one group (FF, Chrome), but it would bring make the website slower for IE users.</p>
<p>According to this current report IE has a worldwide market share of 56 % (with IE8 as most important browser):<br />
<a href="http://thenextweb.com/insider/2013/08/01/ie9-falls-below-10-market-share-firefox-hits-50-month-low-and-chrome-again-gains-the-most/" rel="nofollow">http://thenextweb.com/insider/2013/08/01/ie9-falls-below-10-market-share-firefox-hits-50-month-low-an